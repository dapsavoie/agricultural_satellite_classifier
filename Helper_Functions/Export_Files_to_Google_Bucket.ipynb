{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Export Files to Google Bucket.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9lp8EaB4fu2SKmkgDh3vT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dapsavoie/agricultural_satellite_classifier/blob/master/Export_Files_to_Google_Bucket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AuCb2li36w6",
        "colab_type": "code",
        "outputId": "984ba43a-86d3-4321-97c5-7e97d06acb17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro3t5cn_T5J6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "adb307c1-5ae3-4ce2-e007-d9506dda00af"
      },
      "source": [
        "# set project id to authenticate bucket \n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'stately-planet-278710'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://agri_vision/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5oTdjdIWaOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/content/drive/My Drive/agri_vision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Uvw1nqUnQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bucket_name = 'agri-vision'\n",
        "!gsutil -m cp -r /content/drive/My\\ Drive/agri_vision/ gs://agri_vision/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTT2K_BmInG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bucket_name = 'agri-vision'\n",
        "!gsutil -m cp -r /content/drive/My\\ Drive/agri_vision/Agriculture-Vision/val/ gs://agri_vision/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQrsXyA_mXzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "485a537b-516d-4716-9337-ca10782e09bc"
      },
      "source": [
        "!gsutil help cp"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mNAME\u001b[0;0m\n",
            "  cp - Copy files and objects\n",
            "\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0;0m\n",
            "\n",
            "  gsutil cp [OPTION]... src_url dst_url\n",
            "  gsutil cp [OPTION]... src_url... dst_url\n",
            "  gsutil cp [OPTION]... -I dst_url\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mDESCRIPTION\u001b[0;0m\n",
            "  The gsutil cp command allows you to copy data between your local file\n",
            "  system and the cloud, copy data within the cloud, and copy data between\n",
            "  cloud storage providers. For example, to upload all text files from the\n",
            "  local directory to a bucket you could do:\n",
            "\n",
            "    gsutil cp *.txt gs://my-bucket\n",
            "\n",
            "  Similarly, you can download text files from a bucket by doing:\n",
            "\n",
            "    gsutil cp gs://my-bucket/*.txt .\n",
            "\n",
            "  If you want to copy an entire directory tree you need to use the -r option.\n",
            "  For example, to upload the directory tree \"dir\":\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket\n",
            "\n",
            "  If you have a large number of files to transfer you might want to use the\n",
            "  top-level gsutil -m option (see \"gsutil help options\"), to perform a\n",
            "  parallel (multi-threaded/multi-processing) copy:\n",
            "\n",
            "    gsutil -m cp -r dir gs://my-bucket\n",
            "\n",
            "  You can pass a list of URLs (one per line) to copy on stdin instead of as\n",
            "  command line arguments by using the -I option. This allows you to use gsutil\n",
            "  in a pipeline to upload or download files / objects as generated by a program,\n",
            "  such as:\n",
            "\n",
            "    some_program | gsutil -m cp -I gs://my-bucket\n",
            "\n",
            "  or:\n",
            "\n",
            "    some_program | gsutil -m cp -I ./download_dir\n",
            "\n",
            "  The contents of stdin can name files, cloud URLs, and wildcards of files\n",
            "  and cloud URLs.\n",
            "\n",
            "  NOTE: Shells (like bash, zsh) sometimes attempt to expand wildcards in ways\n",
            "  that can be surprising. Also, attempting to copy files whose names contain\n",
            "  wildcard characters can result in problems. For more details about these\n",
            "  issues see the section \"POTENTIALLY SURPRISING BEHAVIOR WHEN USING WILDCARDS\"\n",
            "  under \"gsutil help wildcards\".\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mHOW NAMES ARE CONSTRUCTED\u001b[0;0m\n",
            "  The gsutil cp command strives to name objects in a way consistent with how\n",
            "  Linux cp works, which causes names to be constructed in varying ways depending\n",
            "  on whether you're performing a recursive directory copy or copying\n",
            "  individually named objects; and whether you're copying to an existing or\n",
            "  non-existent directory.\n",
            "\n",
            "  When performing recursive directory copies, object names are constructed that\n",
            "  mirror the source directory structure starting at the point of recursive\n",
            "  processing. For example, if dir1/dir2 contains the file a/b/c then the\n",
            "  command:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket\n",
            "\n",
            "  will create the object gs://my-bucket/dir2/a/b/c.\n",
            "\n",
            "  In contrast, copying individually named files will result in objects named by\n",
            "  the final path component of the source files. For example, again assuming\n",
            "  dir1/dir2 contains a/b/c, the command:\n",
            "\n",
            "    gsutil cp dir1/dir2/** gs://my-bucket\n",
            "\n",
            "  will create the object gs://my-bucket/c.\n",
            "\n",
            "  The same rules apply for downloads: recursive copies of buckets and\n",
            "  bucket subdirectories produce a mirrored filename structure, while copying\n",
            "  individually (or wildcard) named objects produce flatly named files.\n",
            "\n",
            "  Note that in the above example the '**' wildcard matches all names\n",
            "  anywhere under dir. The wildcard '*' will match names just one level deep. For\n",
            "  more details see \"gsutil help wildcards\".\n",
            "\n",
            "  There's an additional wrinkle when working with subdirectories: the resulting\n",
            "  names depend on whether the destination subdirectory exists. For example,\n",
            "  if gs://my-bucket/subdir exists as a subdirectory, the command:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n",
            "\n",
            "  will create the object gs://my-bucket/subdir/dir2/a/b/c. In contrast, if\n",
            "  gs://my-bucket/subdir does not exist, this same gsutil cp command will create\n",
            "  the object gs://my-bucket/subdir/a/b/c.\n",
            "\n",
            "  NOTE: If you use the\n",
            "  `Google Cloud Platform Console <https://console.cloud.google.com>`_\n",
            "  to create folders, it does so by creating a \"placeholder\" object that ends\n",
            "  with a \"/\" character. gsutil skips these objects when downloading from the\n",
            "  cloud to the local file system, because attempting to create a file that\n",
            "  ends with a \"/\" is not allowed on Linux and macOS. Because of this, it is\n",
            "  recommended that you not create objects that end with \"/\" (unless you don't\n",
            "  need to be able to download such objects using gsutil).\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING TO/FROM SUBDIRECTORIES; DISTRIBUTING TRANSFERS ACROSS MACHINES\u001b[0;0m\n",
            "  You can use gsutil to copy to and from subdirectories by using a command\n",
            "  like:\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket/data\n",
            "\n",
            "  This will cause dir and all of its files and nested subdirectories to be\n",
            "  copied under the specified destination, resulting in objects with names like\n",
            "  gs://my-bucket/data/dir/a/b/c. Similarly you can download from bucket\n",
            "  subdirectories by using a command like:\n",
            "\n",
            "    gsutil cp -r gs://my-bucket/data dir\n",
            "\n",
            "  This will cause everything nested under gs://my-bucket/data to be downloaded\n",
            "  into dir, resulting in files with names like dir/data/a/b/c.\n",
            "\n",
            "  Copying subdirectories is useful if you want to add data to an existing\n",
            "  bucket directory structure over time. It's also useful if you want\n",
            "  to parallelize uploads and downloads across multiple machines (potentially\n",
            "  reducing overall transfer time compared with simply running gsutil -m\n",
            "  cp on one machine). For example, if your bucket contains this structure:\n",
            "\n",
            "    gs://my-bucket/data/result_set_01/\n",
            "    gs://my-bucket/data/result_set_02/\n",
            "    ...\n",
            "    gs://my-bucket/data/result_set_99/\n",
            "\n",
            "  you could perform concurrent downloads across 3 machines by running these\n",
            "  commands on each machine, respectively:\n",
            "\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n",
            "\n",
            "  Note that dir could be a local directory on each machine, or it could be a\n",
            "  directory mounted off of a shared file server; whether the latter performs\n",
            "  acceptably will depend on a number of factors, so we recommend experimenting\n",
            "  to find out what works best for your computing environment.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING IN THE CLOUD AND METADATA PRESERVATION\u001b[0;0m\n",
            "  If both the source and destination URL are cloud URLs from the same\n",
            "  provider, gsutil copies data \"in the cloud\" (i.e., without downloading\n",
            "  to and uploading from the machine where you run gsutil). In addition to\n",
            "  the performance and cost advantages of doing this, copying in the cloud\n",
            "  preserves metadata (like Content-Type and Cache-Control). In contrast,\n",
            "  when you download data from the cloud it ends up in a file, which has\n",
            "  no associated metadata. Thus, unless you have some way to hold on to\n",
            "  or re-create that metadata, downloading to a file will not retain the\n",
            "  metadata.\n",
            "\n",
            "  Copies spanning locations and/or storage classes cause data to be rewritten\n",
            "  in the cloud, which may take some time (but still will be faster than\n",
            "  downloading and re-uploading). Such operations can be resumed with the same\n",
            "  command if they are interrupted, so long as the command parameters are\n",
            "  identical.\n",
            "\n",
            "  Note that by default, the gsutil cp command does not copy the object\n",
            "  ACL to the new object, and instead will use the default bucket ACL (see\n",
            "  \"gsutil help defacl\"). You can override this behavior with the -p\n",
            "  option (see OPTIONS below).\n",
            "\n",
            "  One additional note about copying in the cloud: If the destination bucket has\n",
            "  versioning enabled, by default gsutil cp will copy only live versions of the\n",
            "  source object(s). For example:\n",
            "\n",
            "    gsutil cp gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  will cause only the single live version of gs://bucket1/obj to be copied to\n",
            "  gs://bucket2, even if there are noncurrent versions of gs://bucket1/obj. To\n",
            "  also copy noncurrent versions, use the -A flag:\n",
            "\n",
            "    gsutil cp -A gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  The top-level gsutil -m flag is disallowed when using the cp -A flag, to\n",
            "  ensure that version ordering is preserved.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHECKSUM VALIDATION\u001b[0;0m\n",
            "  At the end of every upload or download the gsutil cp command validates that\n",
            "  the checksum it computes for the source file/object matches the checksum\n",
            "  the service computes. If the checksums do not match, gsutil will delete the\n",
            "  corrupted object and print a warning message. This very rarely happens, but\n",
            "  if it does, please contact gs-team@google.com.\n",
            "\n",
            "  If you know the MD5 of a file before uploading you can specify it in the\n",
            "  Content-MD5 header, which will cause the cloud storage service to reject the\n",
            "  upload if the MD5 doesn't match the value computed by the service. For\n",
            "  example:\n",
            "\n",
            "    % gsutil hash obj\n",
            "    Hashing     obj:\n",
            "    Hashes [base64] for obj:\n",
            "            Hash (crc32c):          lIMoIw==\n",
            "            Hash (md5):             VgyllJgiiaRAbyUUIqDMmw==\n",
            "\n",
            "    % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw== cp obj gs://your-bucket/obj\n",
            "    Copying file://obj [Content-Type=text/plain]...\n",
            "    Uploading   gs://your-bucket/obj:                                182 b/182 B\n",
            "\n",
            "    If the checksum didn't match the service would instead reject the upload and\n",
            "    gsutil would print a message like:\n",
            "\n",
            "    BadRequestException: 400 Provided MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\n",
            "    doesn't match calculated MD5 hash \"7gyllJgiiaRAbyUUIqDMmw==\".\n",
            "\n",
            "  Even if you don't do this gsutil will delete the object if the computed\n",
            "  checksum mismatches, but specifying the Content-MD5 header has several\n",
            "  advantages:\n",
            "\n",
            "  1. It prevents the corrupted object from becoming visible at all, whereas\n",
            "     otherwise it would be visible for 1-3 seconds before gsutil deletes it.\n",
            "\n",
            "  2. If an object already exists with the given name, specifying the\n",
            "     Content-MD5 header will cause the existing object never to be replaced,\n",
            "     whereas otherwise it would be replaced by the corrupted object and then\n",
            "     deleted a few seconds later.\n",
            "\n",
            "  3. It will definitively prevent the corrupted object from being left in\n",
            "     the cloud, whereas the gsutil approach of deleting after the upload\n",
            "     completes could fail if (for example) the gsutil process gets ^C'd\n",
            "     between upload and deletion request.\n",
            "\n",
            "  4. It supports a customer-to-service integrity check handoff. For example,\n",
            "     if you have a content production pipeline that generates data to be\n",
            "     uploaded to the cloud along with checksums of that data, specifying the\n",
            "     MD5 computed by your content pipeline when you run gsutil cp will ensure\n",
            "     that the checksums match all the way through the process (e.g., detecting\n",
            "     if data gets corrupted on your local disk between the time it was written\n",
            "     by your content pipeline and the time it was uploaded to Google Cloud\n",
            "     Storage).\n",
            "\n",
            "  NOTE: The Content-MD5 header is ignored for composite objects, because such\n",
            "  objects only have a CRC32C checksum.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRETRY HANDLING\u001b[0;0m\n",
            "  The cp command will retry when failures occur, but if enough failures happen\n",
            "  during a particular copy or delete operation the cp command will skip that\n",
            "  object and move on. At the end of the copy run if any failures were not\n",
            "  successfully retried, the cp command will report the count of failures, and\n",
            "  exit with non-zero status.\n",
            "\n",
            "  Note that there are cases where retrying will never succeed, such as if you\n",
            "  don't have write permission to the destination bucket or if the destination\n",
            "  path for some objects is longer than the maximum allowed length.\n",
            "\n",
            "  For more details about gsutil's retry handling, please see\n",
            "  \"gsutil help retries\".\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRESUMABLE TRANSFERS\u001b[0;0m\n",
            "  gsutil automatically performs a resumable upload whenever you use the cp\n",
            "  command to upload an object that is larger than 8 MiB. You do not need to\n",
            "  specify any special command line options to make this happen. If your upload\n",
            "  is interrupted you can restart the upload by running the same cp command that\n",
            "  you ran to start the upload. Until the upload has completed successfully, it\n",
            "  will not be visible at the destination object and will not replace any\n",
            "  existing object the upload is intended to overwrite. However, see the section\n",
            "  on parallel composite uploads, which may leave temporary component objects in\n",
            "  place during the upload process.\n",
            "\n",
            "  Similarly, gsutil automatically performs resumable downloads (using standard\n",
            "  HTTP Range GET operations) whenever you use the cp command, unless the\n",
            "  destination is a stream. In this case, a partially downloaded temporary file\n",
            "  will be visible in the destination directory. Upon completion, the original\n",
            "  file is deleted and overwritten with the downloaded contents.\n",
            "\n",
            "  Resumable uploads and downloads store state information in files under\n",
            "  ~/.gsutil, named by the destination object or file. If you attempt to resume a\n",
            "  transfer from a machine with a different directory, the transfer will start\n",
            "  over from scratch.\n",
            "\n",
            "  See also \"gsutil help prod\" for details on using resumable transfers\n",
            "  in production.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSTREAMING TRANSFERS\u001b[0;0m\n",
            "  Use '-' in place of src_url or dst_url to perform a streaming\n",
            "  transfer. For example:\n",
            "\n",
            "    long_running_computation | gsutil cp - gs://my-bucket/obj\n",
            "\n",
            "  Streaming uploads using the JSON API (see \"gsutil help apis\") are buffered in\n",
            "  memory part-way back into the file and can thus retry in the event of network\n",
            "  or service problems.\n",
            "\n",
            "  Streaming transfers using the XML API do not support resumable\n",
            "  uploads/downloads. If you have a large amount of data to upload (say, more\n",
            "  than 100 MiB) it is recommended that you write the data to a local file and\n",
            "  then copy that file to the cloud rather than streaming it (and similarly for\n",
            "  large downloads).\n",
            "\n",
            "  CAUTION: When performing a streaming transfer to or from Cloud Storage,\n",
            "  neither Cloud Storage nor gsutil compute a checksum. If you require data\n",
            "  validation, use a non-streaming transfer, which performs integrity checking\n",
            "  automatically.\n",
            "\n",
            "  NOTE: Streaming transfers are not allowed when the top-level gsutil -m flag\n",
            "  is used.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSLICED OBJECT DOWNLOADS\u001b[0;0m\n",
            "  gsutil uses HTTP Range GET requests to perform \"sliced\" downloads in parallel\n",
            "  when downloading large objects from Google Cloud Storage. This means that disk\n",
            "  space for the temporary download destination file will be pre-allocated and\n",
            "  byte ranges (slices) within the file will be downloaded in parallel. Once all\n",
            "  slices have completed downloading, the temporary file will be renamed to the\n",
            "  destination file. No additional local disk space is required for this\n",
            "  operation.\n",
            "\n",
            "  This feature is only available for Google Cloud Storage objects because it\n",
            "  requires a fast composable checksum (CRC32C) that can be used to verify the\n",
            "  data integrity of the slices. And because it depends on CRC32C, using sliced\n",
            "  object downloads also requires a compiled crcmod (see \"gsutil help crcmod\") on\n",
            "  the machine performing the download. If compiled crcmod is not available,\n",
            "  a non-sliced object download will instead be performed.\n",
            "\n",
            "  NOTE: since sliced object downloads cause multiple writes to occur at various\n",
            "  locations on disk, this mechanism can degrade performance for disks with slow\n",
            "  seek times, especially for large numbers of slices. While the default number\n",
            "  of slices is set small to avoid this problem, you can disable sliced object\n",
            "  download if necessary by setting the \"sliced_object_download_threshold\"\n",
            "  variable in the .boto config file to 0.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mPARALLEL COMPOSITE UPLOADS\u001b[0;0m\n",
            "  gsutil can automatically use\n",
            "  `object composition <https://cloud.google.com/storage/docs/composite-objects>`_\n",
            "  to perform uploads in parallel for large, local files being uploaded to Google\n",
            "  Cloud Storage. If enabled (see below), a large file will be split into\n",
            "  component pieces that are uploaded in parallel and then composed in the cloud\n",
            "  (and the temporary components finally deleted). A file can be broken into as\n",
            "  many as 32 component pieces; until this piece limit is reached, the maximum\n",
            "  size of each component piece is determined by the variable\n",
            "  \"parallel_composite_upload_component_size,\" specified in the [GSUtil] section\n",
            "  of your .boto configuration file (for files that are otherwise too big,\n",
            "  components are as large as needed to fit into 32 pieces). No additional local\n",
            "  disk space is required for this operation.\n",
            "\n",
            "  Using parallel composite uploads presents a tradeoff between upload\n",
            "  performance and download configuration: If you enable parallel composite\n",
            "  uploads your uploads will run faster, but someone will need to install a\n",
            "  compiled crcmod (see \"gsutil help crcmod\") on every machine where objects are\n",
            "  downloaded by gsutil or other Python applications. Note that for such uploads,\n",
            "  crcmod is required for downloading regardless of whether the parallel\n",
            "  composite upload option is on or not. For some distributions this is easy\n",
            "  (e.g., it comes pre-installed on macOS), but in other cases some users have\n",
            "  found it difficult. Because of this, at present parallel composite uploads are\n",
            "  disabled by default. Google is actively working with a number of the Linux\n",
            "  distributions to get crcmod included with the stock distribution. Once that is\n",
            "  done we will re-enable parallel composite uploads by default in gsutil.\n",
            "\n",
            "  WARNING: Parallel composite uploads should not be used with NEARLINE,\n",
            "  COLDLINE, or ARCHIVE storage class buckets, because doing so incurs an early\n",
            "  deletion charge for each component object.\n",
            "  \n",
            "  WARNING: Parallel composite uploads should not be used in buckets that have a\n",
            "  `retention policy <https://cloud.google.com/storage/docs/bucket-lock>`_,\n",
            "  because the component pieces cannot be deleted until each has met the\n",
            "  bucket's minimum retention period.\n",
            "\n",
            "  To try parallel composite uploads you can run the command:\n",
            "\n",
            "    gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp bigfile gs://your-bucket\n",
            "\n",
            "  where bigfile is larger than 150 MiB. When you do this, notice that the upload\n",
            "  progress indicator continuously updates for the file, until all parts of the\n",
            "  upload complete. If after trying this you want to enable parallel composite\n",
            "  uploads for all of your future uploads (notwithstanding the caveats mentioned\n",
            "  earlier), you can uncomment and set the \"parallel_composite_upload_threshold\"\n",
            "  config value in your .boto configuration file to this value.\n",
            "\n",
            "  Note that the crcmod problem only impacts downloads via Python applications\n",
            "  (such as gsutil). If all users who need to download the data using gsutil or\n",
            "  other Python applications can install crcmod, or if no Python users will\n",
            "  need to download your objects, it makes sense to enable parallel composite\n",
            "  uploads (see above). For example, if you use gsutil to upload video assets,\n",
            "  and those assets will only ever be served via a Java application, it would\n",
            "  make sense to enable parallel composite uploads on your machine (there are\n",
            "  efficient CRC32C implementations available in Java).\n",
            "\n",
            "  If a parallel composite upload fails prior to composition, re-running the\n",
            "  gsutil command will take advantage of resumable uploads for the components\n",
            "  that failed, and the component objects will be deleted after the first\n",
            "  successful attempt. Any temporary objects that were uploaded successfully\n",
            "  before gsutil failed will still exist until the upload is completed\n",
            "  successfully. The temporary objects will be named in the following fashion:\n",
            "\n",
            "    <random ID>/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/<hash>\n",
            "\n",
            "  where <random ID> is a numerical value, and <hash> is an MD5 hash (not related\n",
            "  to the hash of the contents of the file or object).\n",
            "\n",
            "  To avoid leaving temporary objects around, you should make sure to check the\n",
            "  exit status from the gsutil command.  This can be done in a bash script, for\n",
            "  example, by doing:\n",
            "\n",
            "    if ! gsutil cp ./local-file gs://your-bucket/your-object; then\n",
            "      << Code that handles failures >>\n",
            "    fi\n",
            "\n",
            "  Or, for copying a directory, use this instead:\n",
            "\n",
            "    if ! gsutil cp -c -L cp.log -r ./dir gs://bucket; then\n",
            "      << Code that handles failures >>\n",
            "    fi\n",
            "\n",
            "  Note that an object uploaded using parallel composite uploads will have a\n",
            "  CRC32C hash, but it will not have an MD5 hash (and because of that, users who\n",
            "  download the object must have crcmod installed, as noted earlier). For details\n",
            "  see \"gsutil help crc32c\".\n",
            "\n",
            "  Parallel composite uploads can be disabled by setting the\n",
            "  \"parallel_composite_upload_threshold\" variable in the .boto config file to 0.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHANGING TEMP DIRECTORIES\u001b[0;0m\n",
            "  gsutil writes data to a temporary directory in several cases:\n",
            "\n",
            "  - when compressing data to be uploaded (see the -z and -Z options)\n",
            "  - when decompressing data being downloaded (when the data has\n",
            "    Content-Encoding:gzip, e.g., as happens when uploaded using gsutil cp -z\n",
            "    or gsutil cp -Z)\n",
            "  - when running integration tests (using the gsutil test command)\n",
            "\n",
            "  In these cases it's possible the temp file location on your system that\n",
            "  gsutil selects by default may not have enough space. If gsutil runs out of\n",
            "  space during one of these operations (e.g., raising\n",
            "  \"CommandException: Inadequate temp space available to compress <your file>\"\n",
            "  during a gsutil cp -z operation), you can change where it writes these\n",
            "  temp files by setting the TMPDIR environment variable. On Linux and macOS\n",
            "  you can do this either by running gsutil this way:\n",
            "\n",
            "    TMPDIR=/some/directory gsutil cp ...\n",
            "\n",
            "  or by adding this line to your ~/.bashrc file and then restarting the shell\n",
            "  before running gsutil:\n",
            "\n",
            "    export TMPDIR=/some/directory\n",
            "\n",
            "  On Windows 7 you can change the TMPDIR environment variable from Start ->\n",
            "  Computer -> System -> Advanced System Settings -> Environment Variables.\n",
            "  You need to reboot after making this change for it to take effect. (Rebooting\n",
            "  is not necessary after running the export command on Linux and macOS.)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSYNCHRONIZING OVER OS-SPECIFIC FILE TYPES (SYMLINKS, DEVICES, ETC.)\u001b[0;0m\n",
            "\n",
            "  Please see the section about OS-specific file types in \"gsutil help rsync\".\n",
            "  While that section was written specifically about the rsync command, analogous\n",
            "  points apply to the cp command.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mOPTIONS\u001b[0;0m\n",
            "  -a canned_acl  Sets named canned_acl when uploaded objects created. See\n",
            "                 \"gsutil help acls\" for further details.\n",
            "\n",
            "  -A             Copy all source versions from a source buckets/folders.\n",
            "                 If not set, only the live version of each source object is\n",
            "                 copied.\n",
            "                 \n",
            "                 NOTE: this option is only useful when the destination\n",
            "                 bucket has versioning enabled.\n",
            "\n",
            "  -c             If an error occurs, continue to attempt to copy the remaining\n",
            "                 files. If any copies were unsuccessful, gsutil's exit status\n",
            "                 will be non-zero even if this flag is set. This option is\n",
            "                 implicitly set when running \"gsutil -m cp...\".\n",
            "                 \n",
            "                 NOTE: -c only applies to the actual copying operation. If an\n",
            "                 error occurs while iterating over the files in the local\n",
            "                 directory (e.g., invalid Unicode file name) gsutil will print\n",
            "                 an error message and abort.\n",
            "\n",
            "  -D             Copy in \"daisy chain\" mode, i.e., copying between two buckets\n",
            "                 by hooking a download to an upload, via the machine where\n",
            "                 gsutil is run. This stands in contrast to the default, where\n",
            "                 data are copied between two buckets \"in the cloud\", i.e.,\n",
            "                 without needing to copy via the machine where gsutil runs.\n",
            "\n",
            "                 By default, a \"copy in the cloud\" when the source is a\n",
            "                 composite object will retain the composite nature of the\n",
            "                 object. However, Daisy chain mode can be used to change a\n",
            "                 composite object into a non-composite object. For example:\n",
            "\n",
            "                     gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\n",
            "                     gsutil mv -p gs://bucket/obj_tmp gs://bucket/obj\n",
            "\n",
            "                 NOTE: Daisy chain mode is automatically used when copying\n",
            "                 between providers (e.g., to copy data from Google Cloud Storage\n",
            "                 to another provider).\n",
            "\n",
            "  -e             Exclude symlinks. When specified, symbolic links will not be\n",
            "                 copied.\n",
            "\n",
            "  -I             Causes gsutil to read the list of files or objects to copy from\n",
            "                 stdin. This allows you to run a program that generates the list\n",
            "                 of files to upload/download.\n",
            "\n",
            "  -j <ext,...>   Applies gzip transport encoding to any file upload whose\n",
            "                 extension matches the -j extension list. This is useful when\n",
            "                 uploading files with compressible content (such as .js, .css,\n",
            "                 or .html files) because it saves network bandwidth while\n",
            "                 also leaving the data uncompressed in Google Cloud Storage.\n",
            "\n",
            "                 When you specify the -j option, files being uploaded are\n",
            "                 compressed in-memory and on-the-wire only. Both the local\n",
            "                 files and Cloud Storage objects remain uncompressed. The\n",
            "                 uploaded objects retain the Content-Type and name of the\n",
            "                 original files.\n",
            "\n",
            "                 Note that if you want to use the top-level -m option to\n",
            "                 parallelize copies along with the -j/-J options, your\n",
            "                 performance may be bottlenecked by the\n",
            "                 \"max_upload_compression_buffer_size\" boto config option,\n",
            "                 which is set to 2 GiB by default. This compression buffer\n",
            "                 size can be changed to a higher limit, e.g.:\n",
            "\n",
            "                   gsutil -o \"GSUtil:max_upload_compression_buffer_size=8G\" \\\n",
            "                     -m cp -j html -r /local/source/dir gs://bucket/path\n",
            "\n",
            "  -J             Applies gzip transport encoding to file uploads. This option\n",
            "                 works like the -j option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If you use this option and some of the source files\n",
            "                 don't compress well (e.g., that's often true of binary data),\n",
            "                 this option may result in longer uploads.\n",
            "\n",
            "  -L <file>      Outputs a manifest log file with detailed information about\n",
            "                 each item that was copied. This manifest contains the following\n",
            "                 information for each item:\n",
            "\n",
            "                 - Source path.\n",
            "                 - Destination path.\n",
            "                 - Source size.\n",
            "                 - Bytes transferred.\n",
            "                 - MD5 hash.\n",
            "                 - UTC date and time transfer was started in ISO 8601 format.\n",
            "                 - UTC date and time transfer was completed in ISO 8601 format.\n",
            "                 - Upload id, if a resumable upload was performed.\n",
            "                 - Final result of the attempted transfer, success or failure.\n",
            "                 - Failure details, if any.\n",
            "\n",
            "                 If the log file already exists, gsutil will use the file as an\n",
            "                 input to the copy process, and will also append log items to\n",
            "                 the existing file. Files/objects that are marked in the\n",
            "                 existing log file as having been successfully copied (or\n",
            "                 skipped) will be ignored. Files/objects without entries will be\n",
            "                 copied and ones previously marked as unsuccessful will be\n",
            "                 retried. This can be used in conjunction with the -c option to\n",
            "                 build a script that copies a large number of objects reliably,\n",
            "                 using a bash script like the following:\n",
            "\n",
            "                   until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n",
            "                     sleep 1\n",
            "                   done\n",
            "\n",
            "                 The -c option will cause copying to continue after failures\n",
            "                 occur, and the -L option will allow gsutil to pick up where it\n",
            "                 left off without duplicating work. The loop will continue\n",
            "                 running as long as gsutil exits with a non-zero status (such a\n",
            "                 status indicates there was at least one failure during the\n",
            "                 gsutil run).\n",
            "\n",
            "                 NOTE: If you're trying to synchronize the contents of a\n",
            "                 directory and a bucket (or two buckets), see\n",
            "                 \"gsutil help rsync\".\n",
            "\n",
            "  -n             No-clobber. When specified, existing files or objects at the\n",
            "                 destination will not be overwritten. Any items that are skipped\n",
            "                 by this option will be reported as being skipped. This option\n",
            "                 will perform an additional GET request to check if an item\n",
            "                 exists before attempting to upload the data. This will save\n",
            "                 retransmitting data, but the additional HTTP requests may make\n",
            "                 small object transfers slower and more expensive.\n",
            "\n",
            "  -p             Causes ACLs to be preserved when copying in the cloud. Note\n",
            "                 that this option has performance and cost implications when\n",
            "                 using  the XML API, as it requires separate HTTP calls for\n",
            "                 interacting with ACLs. (There are no such performance or cost\n",
            "                 implications when using the -p option with the JSON API.) The\n",
            "                 performance issue can be mitigated to some degree by using\n",
            "                 gsutil -m cp to cause parallel copying. Note that this option\n",
            "                 only works if you have OWNER access to all of the objects that\n",
            "                 are copied.\n",
            "\n",
            "                 You can avoid the additional performance and cost of using\n",
            "                 cp -p if you want all objects in the destination bucket to end\n",
            "                 up with the same ACL by setting a default object ACL on that\n",
            "                 bucket instead of using cp -p. See \"gsutil help defacl\".\n",
            "\n",
            "                 Note that it's not valid to specify both the -a and -p options\n",
            "                 together.\n",
            "\n",
            "  -P             Causes POSIX attributes to be preserved when objects are\n",
            "                 copied. With this feature enabled, gsutil cp will copy fields\n",
            "                 provided by stat. These are the user ID of the owner, the group\n",
            "                 ID of the owning group, the mode (permissions) of the file, and\n",
            "                 the access/modification time of the file. For downloads, these\n",
            "                 attributes will only be set if the source objects were uploaded\n",
            "                 with this flag enabled.\n",
            "\n",
            "                 On Windows, this flag will only set and restore access time and\n",
            "                 modification time. This is because Windows doesn't have a\n",
            "                 notion of POSIX uid/gid/mode.\n",
            "\n",
            "  -R, -r         The -R and -r options are synonymous. Causes directories,\n",
            "                 buckets, and bucket subdirectories to be copied recursively.\n",
            "                 If you neglect to use this option for an upload, gsutil will\n",
            "                 copy any files it finds and skip any directories. Similarly,\n",
            "                 neglecting to specify this option for a download will cause\n",
            "                 gsutil to copy any objects at the current bucket directory\n",
            "                 level, and skip any subdirectories.\n",
            "\n",
            "  -s <class>     The storage class of the destination object(s). If not\n",
            "                 specified, the default storage class of the destination bucket\n",
            "                 is used. Not valid for copying to non-cloud destinations.\n",
            "\n",
            "  -U             Skip objects with unsupported object types instead of failing.\n",
            "                 Unsupported object types are Amazon S3 Objects in the GLACIER\n",
            "                 storage class.\n",
            "\n",
            "  -v             Requests that the version-specific URL for each uploaded object\n",
            "                 be printed. Given this URL you can make future upload requests\n",
            "                 that are safe in the face of concurrent updates, because Google\n",
            "                 Cloud Storage will refuse to perform the update if the current\n",
            "                 object version doesn't match the version-specific URL. See\n",
            "                 \"gsutil help versions\" for more details.\n",
            "\n",
            "  -z <ext,...>   Applies gzip content-encoding to any file upload whose\n",
            "                 extension matches the -z extension list. This is useful when\n",
            "                 uploading files with compressible content (such as .js, .css,\n",
            "                 or .html files) because it saves network bandwidth and space\n",
            "                 in Google Cloud Storage, which in turn reduces storage costs.\n",
            "\n",
            "                 When you specify the -z option, the data from your files is\n",
            "                 compressed before it is uploaded, but your actual files are\n",
            "                 left uncompressed on the local disk. The uploaded objects\n",
            "                 retain the Content-Type and name of the original files but are\n",
            "                 given a Content-Encoding header with the value \"gzip\" to\n",
            "                 indicate that the object data stored are compressed on the\n",
            "                 Google Cloud Storage servers.\n",
            "\n",
            "                 For example, the following command:\n",
            "\n",
            "                   gsutil cp -z html -a public-read \\\n",
            "                     cattypes.html tabby.jpeg gs://mycats\n",
            "\n",
            "                 will do all of the following:\n",
            "\n",
            "                 - Upload the files cattypes.html and tabby.jpeg to the bucket\n",
            "                   gs://mycats (cp command)\n",
            "                 - Set the Content-Type of cattypes.html to text/html and\n",
            "                   tabby.jpeg to image/jpeg (based on file extensions)\n",
            "                 - Compress the data in the file cattypes.html (-z option)\n",
            "                 - Set the Content-Encoding for cattypes.html to gzip\n",
            "                   (-z option)\n",
            "                 - Set the ACL for both files to public-read (-a option)\n",
            "                 - If a user tries to view cattypes.html in a browser, the\n",
            "                   browser will know to uncompress the data based on the\n",
            "                   Content-Encoding header and to render it as HTML based on\n",
            "                   the Content-Type header.\n",
            "\n",
            "                 Because the -z/-Z options compress data prior to upload, they\n",
            "                 are not subject to the same compression buffer bottleneck that\n",
            "                 can affect the -j/-J options.\n",
            "\n",
            "                 Note that if you download an object with Content-Encoding:gzip\n",
            "                 gsutil decompresses the content before writing the local file.\n",
            "\n",
            "  -Z             Applies gzip content-encoding to file uploads. This option\n",
            "                 works like the -z option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If you use this option and some of the source files\n",
            "                 don't compress well (e.g., that's often true of binary data),\n",
            "                 this option may result in files taking up more space in the\n",
            "                 cloud than they would if left uncompressed."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
